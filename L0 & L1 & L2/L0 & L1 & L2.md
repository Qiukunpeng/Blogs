<!--
 * @Author: Qiu Kunpeng
 * @Date: 2021-01-21 12:56:01
 * @LastEditTime: 2021-01-24 21:36:28
 * @LastEditors: Please set LastEditors
 * @Description: This blog is about L0-norm L1-norm and L2-norm.
 * @FilePath: \L0 & L1 & L2\L0 & L1 & L2.md
-->
# L0 & L1 & L2 Norms
## What is L0 & L1 & L2 Norm?
- When the over-fitting happens in **Deep Learning** models, **regularization** is often chosen to alleviate or solve the problem. Thus, in **Machine Learning**, almost all the **Loss Functions** have an extra item. Generally, there are two kinds of this item, one is ***L1-norm***, the other one is ***L2-norm***. (**Norm** can be replaced by **regularization**)
- The **norm** can be taken as the **Punishment** of **Loss Function**. Actually, it only makes some restrictions on some specific parameters. For example, ***w***, which is the weights of the model.
- The goals of the **norm** or **regularization** are to restrict the **complexity** of the model, to reduce the probability of **over-fitting**.
- The item consists of a constant which is important to the performance of the **regularization**.
- The specific representation **norm** of **Matrix** and **Vector** are different.
## The norm of Matrix
- To make better explanation of the norm of **Matrix**, we can take an example as an instance.
$$
    A = 
    \left[
    \begin{matrix}
    0 & 2 & -3\\
    4 & 0 & 6\\
    1 & -5 & 0
    \end{matrix}
    \right] \tag{1}
$$
### L0-norm
1. **L0-norm** is the number of non-zero digital. We can use **L0-norm** to represent the **sparsity**. That means the smaller the number the sparser. The number of matrix (1) is 6.
### L1-norm
1. The mathematical expression of the **L1-norm** is:
$$
    ||A||_1 = \underset{X\neq0}{max}\frac{||AX||_1}{||X||_1} = \underset{1\leq{j}\leq{n}}{max}\sum_{i=1}^{n}|a_{ij}| \tag{2}
$$
2. The mathematic above means we have to sum the absolute value of each column in the **Matrix** and then return the **Max value** of all the column. In matrix (1), we can get 5, 7 and 9 first. Obviously, 9 is the largest one. Thus, the result of $||A||_1$ is 9.
### L2-norm
1. The mathematical expression of the **L1-norm** is:
$$
    ||A||_2 = \underset{X\neq0}{max}\frac{||AX||_2}{||X||_2} = \sqrt{\lambda_{max}(A^TA)} = \sqrt{\underset{1\leq{i}\leq{n}}{max}|\lambda|} \tag{3}
$$
2. The mathematic above means we have to calculate the product of the transpose matrix of A times A matrix. And then, we have to calculate the **Eigenvalues** which is **$\lambda$** in the equation. We only choose the largest one called **$\lambda_{max}$** and calculate the **sqrt value**. In matrix (1), **$\lambda_{max}$** is shown below:

$$
## The norm of Vector
- The mathematical expression of the **Vector norm** is shown below and p is a natural number.
$$
    ||X||_0 = (\sum_{i=1}^n|x_i|^0), if p=0 \tag{2}
$$
$$
    ||X||_p = (\sum_{i=1}^n|x_i|^p)^\frac{1}{p}, if p>0 \tag{3}
$$